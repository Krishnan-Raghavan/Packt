{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:32:54.303189Z","iopub.execute_input":"2024-04-16T04:32:54.303572Z","iopub.status.idle":"2024-04-16T04:32:55.359130Z","shell.execute_reply.started":"2024-04-16T04:32:54.303517Z","shell.execute_reply":"2024-04-16T04:32:55.358224Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Tue Apr 16 04:32:55 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   45C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install diffusers\n!pip install transformers scipy ftfy accelerate ipywidgets","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:32:59.294643Z","iopub.execute_input":"2024-04-16T04:32:59.295420Z","iopub.status.idle":"2024-04-16T04:33:30.388072Z","shell.execute_reply.started":"2024-04-16T04:32:59.295386Z","shell.execute_reply":"2024-04-16T04:33:30.387098Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting diffusers\n  Downloading diffusers-0.27.2-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers) (6.11.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers) (3.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.2 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.22.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers) (2.31.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.4.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers) (9.5.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.2->diffusers) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.2->diffusers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.2->diffusers) (6.0.1)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.2->diffusers) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.2->diffusers) (4.9.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (2024.2.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.2->diffusers) (3.1.1)\nDownloading diffusers-0.27.2-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: diffusers\nSuccessfully installed diffusers-0.27.2\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.4)\nCollecting ftfy\n  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.28.0)\nRequirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (7.7.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.13)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (6.28.0)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.0)\nRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (3.6.6)\nRequirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.20.0)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (3.0.9)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.2.1)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.0)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.7.1)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.8)\nRequirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\nRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (0.19.1)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.42)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (2.17.2)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.10/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\nRequirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (4.2.0)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\nRequirement already satisfied: nbformat in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.9.2)\nRequirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.4.5)\nRequirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.2)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.0)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.19.0)\nRequirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.0.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=4.0.0->ipywidgets) (1.16.0)\nRequirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.13.0)\nRequirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.3)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.1.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.0)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.2)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.13)\nRequirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.19.1)\nRequirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.20.0)\nRequirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.16.2)\nRequirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.2.0)\nRequirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.9.0)\nRequirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\nRequirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.4.0)\nRequirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.7.0)\nRequirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.5)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\nRequirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.7)\nRequirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.1)\nRequirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\nRequirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.4)\nRequirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\nRequirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.13)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.8.19.20240106)\nDownloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m775.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torch torchvision torchaudio\n!pip install bitsandbytes\n!pip install peft\n!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:33:46.296072Z","iopub.execute_input":"2024-04-16T04:33:46.296420Z","iopub.status.idle":"2024-04-16T04:34:41.097203Z","shell.execute_reply.started":"2024-04-16T04:33:46.296392Z","shell.execute_reply":"2024-04-16T04:34:41.096022Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\nCollecting peft\n  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.28.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.22.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.10.0\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nw_list = np.array([2,3,4,7])\n\nimport random\nx_list = []\nfor _ in range(10):\n    x_sample = np.array([random.randint(1,100) for _ in range(len(w_list))])\n    x_list.append(x_sample)\n\ny_list = []\nfor x_sample in x_list:\n    y_temp = x_sample@w_list\n    y_list.append(y_temp)\n\nprint(\"x_list:\",x_list)\nprint(\"y_list:\",y_list)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:35:01.935565Z","iopub.execute_input":"2024-04-16T04:35:01.935931Z","iopub.status.idle":"2024-04-16T04:35:01.944980Z","shell.execute_reply.started":"2024-04-16T04:35:01.935901Z","shell.execute_reply":"2024-04-16T04:35:01.943991Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"x_list: [array([94, 97, 12, 58]), array([86, 69, 22, 37]), array([50, 17, 45, 72]), array([18, 38, 26, 63]), array([34, 42, 10, 68]), array([83, 77, 30, 92]), array([69, 81, 41,  9]), array([40, 32, 15, 88]), array([60,  3, 39, 77]), array([98, 43, 12,  3])]\ny_list: [933, 726, 835, 695, 710, 1161, 608, 852, 824, 394]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\n\nclass MyLinear(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.w = nn.Parameter(torch.randn(4))\n\n    def forward(self, x:torch.Tensor):\n        return self.w @ x","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:35:14.699944Z","iopub.execute_input":"2024-04-16T04:35:14.700282Z","iopub.status.idle":"2024-04-16T04:35:20.698128Z","shell.execute_reply.started":"2024-04-16T04:35:14.700255Z","shell.execute_reply":"2024-04-16T04:35:20.697291Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model = MyLinear()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:35:29.606521Z","iopub.execute_input":"2024-04-16T04:35:29.607451Z","iopub.status.idle":"2024-04-16T04:35:29.654193Z","shell.execute_reply.started":"2024-04-16T04:35:29.607420Z","shell.execute_reply":"2024-04-16T04:35:29.653503Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = 0.00001)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:35:33.789412Z","iopub.execute_input":"2024-04-16T04:35:33.790273Z","iopub.status.idle":"2024-04-16T04:35:37.513837Z","shell.execute_reply.started":"2024-04-16T04:35:33.790240Z","shell.execute_reply":"2024-04-16T04:35:37.513039Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"x_input = torch.tensor(x_list, dtype=torch.float32)\ny_output = torch.tensor(y_list, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:35:40.154282Z","iopub.execute_input":"2024-04-16T04:35:40.155113Z","iopub.status.idle":"2024-04-16T04:35:40.165200Z","shell.execute_reply.started":"2024-04-16T04:35:40.155083Z","shell.execute_reply":"2024-04-16T04:35:40.164211Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/198170575.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  x_input = torch.tensor(x_list, dtype=torch.float32)\n","output_type":"stream"}]},{"cell_type":"code","source":"# start train model\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    for i, x in enumerate(x_input):\n        # forward\n        y_pred = model(x)\n\n        # calculate loss\n        loss = loss_fn(y_pred,y_output[i])\n\n        # zero out the cached parameter.\n        optimizer.zero_grad()\n\n        # backward\n        loss.backward()\n\n        # update paramters\n        optimizer.step()\n\n    if (epoch+1) % 10 == 0:\n        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n\nprint(\"train done\") ","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:35:56.185265Z","iopub.execute_input":"2024-04-16T04:35:56.185645Z","iopub.status.idle":"2024-04-16T04:35:56.780999Z","shell.execute_reply.started":"2024-04-16T04:35:56.185617Z","shell.execute_reply":"2024-04-16T04:35:56.780065Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch [10/100], Loss: 3520.0327\nEpoch [20/100], Loss: 792.9136\nEpoch [30/100], Loss: 351.4698\nEpoch [40/100], Loss: 165.5834\nEpoch [50/100], Loss: 78.4869\nEpoch [60/100], Loss: 37.2611\nEpoch [70/100], Loss: 17.7042\nEpoch [80/100], Loss: 8.4165\nEpoch [90/100], Loss: 4.0024\nEpoch [100/100], Loss: 1.9037\ntrain done\n","output_type":"stream"}]},{"cell_type":"code","source":"model.w","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:36:04.319724Z","iopub.execute_input":"2024-04-16T04:36:04.320585Z","iopub.status.idle":"2024-04-16T04:36:04.366705Z","shell.execute_reply.started":"2024-04-16T04:36:04.320552Z","shell.execute_reply":"2024-04-16T04:36:04.365619Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Parameter containing:\ntensor([2.0250, 2.9863, 3.9298, 7.0135], requires_grad=True)"},"metadata":{}}]},{"cell_type":"code","source":"from accelerate import utils\nutils.write_basic_config()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:36:23.879578Z","iopub.execute_input":"2024-04-16T04:36:23.880421Z","iopub.status.idle":"2024-04-16T04:36:24.409364Z","shell.execute_reply.started":"2024-04-16T04:36:23.880389Z","shell.execute_reply":"2024-04-16T04:36:24.408431Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"PosixPath('/root/.cache/huggingface/accelerate/default_config.yaml')"},"metadata":{}}]},{"cell_type":"code","source":"from accelerate import utils\nutils.write_basic_config()\n\nfrom accelerate import Accelerator\naccelerator = Accelerator()\ndevice = accelerator.device\n\nx_input.to(device)\ny_output.to(device)\nmodel.to(device)\n\nmodel, optimizer = accelerator.prepare(\n    model, optimizer\n)\n\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    for i, x in enumerate(x_input):\n        # forward\n        y_pred = model(x)\n\n        # calculate loss\n        loss = loss_fn(y_pred,y_output[i])\n\n        # zero out the cached parameter.\n        optimizer.zero_grad()\n\n        # backward\n        #loss.backward()\n        accelerator.backward(loss)\n\n        # update paramters\n        optimizer.step()\n\n    if (epoch+1) % 10 == 0:\n        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n\nprint(\"train done\") ","metadata":{"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"MyLinear()"},"metadata":{}}]},{"cell_type":"code","source":"model = accelerator.unwrap_model(model)\nmodel.w","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:39:31.574360Z","iopub.execute_input":"2024-04-16T04:39:31.575215Z","iopub.status.idle":"2024-04-16T04:39:32.062554Z","shell.execute_reply.started":"2024-04-16T04:39:31.575170Z","shell.execute_reply":"2024-04-16T04:39:32.061578Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Parameter containing:\ntensor([2.0250, 2.9863, 3.9298, 7.0135], device='cuda:0', requires_grad=True)"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nw_list = np.array([2,3,4,7])\n\nimport random\nx_list = []\nfor _ in range(10):\n    x_sample = np.array([random.randint(1,100) for _ in range(len(w_list))])\n    x_list.append(x_sample)\n\ny_list = []\nfor x_sample in x_list:\n    y_temp = x_sample@w_list\n    y_list.append(y_temp)\ntrain_obj = {\n    'w_list':w_list.tolist()\n    , 'input':x_list\n    , 'output':y_list\n}\n\nimport pickle\nwith open('train_data.pkl','wb') as f:\n    pickle.dump(train_obj,f)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:40:13.035363Z","iopub.execute_input":"2024-04-16T04:40:13.036141Z","iopub.status.idle":"2024-04-16T04:40:13.043731Z","shell.execute_reply.started":"2024-04-16T04:40:13.036103Z","shell.execute_reply":"2024-04-16T04:40:13.042748Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom accelerate import utils\nfrom accelerate import Accelerator\n\n# start a accelerate instance\nutils.write_basic_config()\naccelerator = Accelerator()\ndevice = accelerator.device\n\ndef main():\n    # define the model\n    class MyLinear(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.w = nn.Parameter(torch.randn(len(w_list)))\n        \n        def forward(self, x:torch.Tensor):\n            return self.w @ x\n\n    # load training data\n    import pickle\n    with open(\"train_data.pkl\",'rb') as f:\n        loaded_object = pickle.load(f)\n    w_list = loaded_object['w_list']\n    x_list = loaded_object['input']\n    y_list = loaded_object['output']\n\n    # convert data to torch tensor\n    x_input     = torch.tensor(x_list, dtype=torch.float32).to(device)\n    y_output    = torch.tensor(y_list, dtype=torch.float32).to(device)\n\n    # initialize model, loss function, and optimizer\n    model       = MyLinear().to(device)\n    loss_fn     = nn.MSELoss()\n    optimizer   = torch.optim.SGD(model.parameters(), lr = 0.00001)\n\n    # wrap model and optimizer using accelerate\n    model, optimizer = accelerator.prepare(\n        model, optimizer\n    )\n\n    num_epochs = 100\n    for epoch in range(num_epochs):\n        for i, x in enumerate(x_input):\n            # forward\n            y_pred = model(x)\n\n            # calculate loss\n            loss = loss_fn(y_pred,y_output[i])\n\n            # zero out the cached parameter.\n            optimizer.zero_grad()\n\n            # backward\n            #loss.backward()\n            accelerator.backward(loss)\n\n            # update paramters\n            optimizer.step()\n\n        if (epoch+1) % 10 == 0:\n            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n    \n    # take a look at the model weights after trainning\n    model = accelerator.unwrap_model(model)\n    print(model.w)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:41:53.494656Z","iopub.execute_input":"2024-04-16T04:41:53.495030Z","iopub.status.idle":"2024-04-16T04:41:54.529415Z","shell.execute_reply.started":"2024-04-16T04:41:53.495002Z","shell.execute_reply":"2024-04-16T04:41:54.528398Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\nEpoch [10/100], Loss: 124.7672\nEpoch [20/100], Loss: 158.8025\nEpoch [30/100], Loss: 91.4274\nEpoch [40/100], Loss: 39.8153\nEpoch [50/100], Loss: 15.3667\nEpoch [60/100], Loss: 5.5932\nEpoch [70/100], Loss: 1.9755\nEpoch [80/100], Loss: 0.6868\nEpoch [90/100], Loss: 0.2368\nEpoch [100/100], Loss: 0.0812\nParameter containing:\ntensor([2.0098, 3.0060, 3.9959, 6.9913], device='cuda:0', requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"# import packages\nimport torch\nfrom accelerate import utils\nfrom accelerate import Accelerator\nfrom diffusers import DDPMScheduler,StableDiffusionPipeline\nfrom peft import LoraConfig\nfrom peft.utils import get_peft_model_state_dict\nfrom datasets import load_dataset\nfrom torchvision import transforms\nimport math\nfrom diffusers.optimization import get_scheduler\nfrom tqdm.auto import tqdm\nimport torch.nn.functional as F\nfrom diffusers.utils import convert_state_dict_to_diffusers\n\nfrom datetime import datetime\nformatted_date = datetime.now().strftime(r'%Y%m%d-%H%M%S')   \n\n# train code \ndef main():\n    utils.write_basic_config()\n\n    # hyperparameters\n    output_dir                      = \".\"\n    pretrained_model_name_or_path   = \"runwayml/stable-diffusion-v1-5\"\n    lora_rank                       = 4\n    lora_alpha                      = 4\n    learning_rate                   = 1e-4\n    adam_beta1, adam_beta2          = 0.9, 0.999\n    adam_weight_decay               = 1e-2\n    adam_epsilon                    = 1e-08\n    dataset_name                    = None                  #\"lambdalabs/pokemon-blip-captions\"\n    train_data_dir                  = \"./train_data\"\n    top_rows                        = 4\n    output_dir                      = \"output_dir\"\n    resolution                      = 768\n    center_crop                     = True\n    random_flip                     = True\n    train_batch_size                = 4\n    gradient_accumulation_steps     = 1\n    num_train_epochs                = 200\n    # The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",\"constant\", \"constant_with_warmup\"]   \n    lr_scheduler_name               = \"constant\" #\"cosine\"#\n    max_grad_norm                   = 1.0\n    diffusion_scheduler             = DDPMScheduler  \n\n    accelerator = Accelerator(\n        gradient_accumulation_steps = gradient_accumulation_steps\n        , mixed_precision           = \"fp16\" \n    )\n    device      = accelerator.device\n\n    # Load scheduler, tokenizer and unet models.\n    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n    weight_dtype = torch.float16\n    pipe = StableDiffusionPipeline.from_pretrained(\n        pretrained_model_name_or_path, \n        torch_dtype = weight_dtype\n    ).to(device)\n    tokenizer,text_encoder, vae, unet   = pipe.tokenizer, pipe.text_encoder, pipe.vae, pipe.unet\n\n    # freeze parameters of models, we just want to train a LoRA only\n    unet.requires_grad_(False)\n    vae.requires_grad_(False)\n    text_encoder.requires_grad_(False)\n\n    # configure LoRA parameters use PEFT\n    unet_lora_config = LoraConfig(\n        r                     = lora_rank\n        , lora_alpha          = lora_alpha\n        , init_lora_weights   = \"gaussian\"\n        , target_modules      = [\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"]\n    )\n\n    # Add adapter and make sure the trainable params are in float32.\n    unet.add_adapter(unet_lora_config)\n    for param in unet.parameters():\n        # only upcast trainable parameters (LoRA) into fp32\n        if param.requires_grad:\n            param.data = param.to(torch.float32)\n    \n    # Downloading and loading a dataset from the hub. data will be saved to ~/.cache/huggingface/datasets by default\n    if dataset_name:\n        dataset = load_dataset(dataset_name)\n    else:\n        dataset = load_dataset(\n            \"imagefolder\"\n            , data_dir = train_data_dir\n        )\n    \n    train_data = dataset[\"train\"]\n    dataset[\"train\"] = train_data.select(range(top_rows))\n    print(dataset[\"train\"])\n\n    # Preprocessing the datasets. We need to tokenize inputs and targets.\n    dataset_columns = list(dataset[\"train\"].features.keys())\n    image_column, caption_column = dataset_columns[0],dataset_columns[1]\n\n    def tokenize_captions(examples, is_train=True):\n        '''Preprocessing the datasets.We need to tokenize input captions and transform the images.'''\n        captions = []\n        for caption in examples[caption_column]:\n            if isinstance(caption, str):\n                captions.append(caption)\n        inputs = tokenizer(\n            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n        )\n        return inputs.input_ids\n\n    # Preprocessing the datasets.\n    train_transforms = transforms.Compose(\n        [\n            transforms.Resize(\n                resolution\n                , interpolation=transforms.InterpolationMode.BILINEAR\n            )\n            , transforms.CenterCrop(resolution) if center_crop else transforms.RandomCrop(resolution)\n            , transforms.RandomHorizontalFlip() if random_flip else transforms.Lambda(lambda x: x)\n            , transforms.ToTensor()\n            , transforms.Normalize([0.5], [0.5]) # [0,1] -> [-1,1]\n        ]\n    )\n\n    def preprocess_train(examples):\n        '''prepare the train data'''\n        images                      = [image.convert(\"RGB\") for image in examples[image_column]]\n        examples[\"pixel_values\"]    = [train_transforms(image) for image in images]\n        examples[\"input_ids\"]       = tokenize_captions(examples)\n        return examples\n\n    # only do this in the main process\n    with accelerator.main_process_first():\n        # Set the training transforms\n        train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n    \n    def collate_fn(examples):\n        pixel_values    = torch.stack([example[\"pixel_values\"] for example in examples])\n        pixel_values    = pixel_values.to(memory_format = torch.contiguous_format).float()\n        input_ids       = torch.stack([example[\"input_ids\"] for example in examples])\n        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n    \n    # DataLoaders creation:\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset\n        , shuffle       = True\n        , collate_fn    = collate_fn\n        , batch_size    = train_batch_size\n        , num_workers   = 0\n    )\n\n    print(\"Data Size:\",len(train_dataloader))\n\n    #num_update_steps_per_epoch  = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n    num_update_steps_per_epoch  = math.ceil(len(train_dataloader) / train_batch_size)\n    max_train_steps             = num_train_epochs * num_update_steps_per_epoch\n\n    # initialize optimizer\n    lora_layers = filter(lambda p: p.requires_grad, unet.parameters())\n    optimizer = torch.optim.AdamW(\n        lora_layers\n        , lr            = learning_rate\n        , betas         = (adam_beta1, adam_beta2)\n        , weight_decay  = adam_weight_decay\n        , eps           = adam_epsilon\n    )\n\n    # learn rate scheduler from diffusers's get_scheduler\n    lr_scheduler = get_scheduler(\n        lr_scheduler_name\n        , optimizer             = optimizer\n    )\n\n    # Prepare everything with our `accelerator`.\n    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n        unet, optimizer, train_dataloader, lr_scheduler\n    )\n\n    # set step count and progress bar\n    max_train_steps = num_train_epochs*len(train_dataloader)\n    progress_bar = tqdm(\n        range(0, max_train_steps)\n        , initial   = 0\n        , desc      = \"Steps\"\n        # Only show the progress bar once on each machine.\n        , disable   = not accelerator.is_local_main_process,\n    )\n\n    # start train\n    for epoch in range(num_train_epochs):\n        unet.train()\n        train_loss = 0.0\n        for step, batch in enumerate(train_dataloader):\n            # step 1. Convert images to latent space\n            latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n            latents = latents * vae.config.scaling_factor\n\n            # step 2. Sample noise that we'll add to the latents, latents provide the shape info. \n            noise = torch.randn_like(latents)\n\n            # step 3. Sample a random timestep for each image\n            batch_size = latents.shape[0]\n            timesteps = torch.randint(\n                low         = 0\n                , high      = noise_scheduler.config.num_train_timesteps\n                , size      = (batch_size,)\n                , device    = latents.device\n            )\n            timesteps = timesteps.long()\n\n            # step 4. Get the text embedding for conditioning\n            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n\n            # step 5. Add noise to the latents according to the noise magnitude at each timestep\n            # (this is the forward diffusion process), provide to unet to get the prediction result\n            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n\n            # step 6. Get the target for loss depend on the prediction type\n            if noise_scheduler.config.prediction_type == \"epsilon\":\n                target = noise\n            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n                target = noise_scheduler.get_velocity(latents, noise, timesteps)\n            else:\n                raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n\n            # step 7. Predict the noise residual and compute loss\n            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n\n            # step 8. Calculate loss\n            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n\n            # step 9. Gather the losses across all processes for logging (if we use distributed training).\n            avg_loss = accelerator.gather(loss.repeat(train_batch_size)).mean()\n            train_loss += avg_loss.item() / gradient_accumulation_steps\n\n            # step 10. Backpropagate\n            accelerator.backward(loss)\n            if accelerator.sync_gradients:\n                params_to_clip = lora_layers\n                accelerator.clip_grad_norm_(params_to_clip, max_grad_norm)\n            \n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n            # step 11. check optimization step and update progress bar\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                train_loss = 0.0\n            \n            logs = {\"epoch\": epoch,\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n            progress_bar.set_postfix(**logs)\n\n    # Save the lora layers\n    accelerator.wait_for_everyone()\n    if accelerator.is_main_process:\n        unet = unet.to(torch.float32)\n\n        unwrapped_unet = accelerator.unwrap_model(unet)\n        unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))\n\n        weight_name = f\"lora_{pretrained_model_name_or_path.split('/')[-1]}_rank{lora_rank}_s{max_train_steps}_r{resolution}_{diffusion_scheduler.__name__}_{formatted_date}.safetensors\"   \n        StableDiffusionPipeline.save_lora_weights(\n            save_directory          = output_dir\n            , unet_lora_layers      = unet_lora_state_dict\n            , safe_serialization    = True\n            , weight_name           = weight_name\n        )\n\n    accelerator.end_training()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:42:35.985903Z","iopub.execute_input":"2024-04-16T04:42:35.986438Z","iopub.status.idle":"2024-04-16T04:42:56.999793Z","shell.execute_reply.started":"2024-04-16T04:42:35.986404Z","shell.execute_reply":"2024-04-16T04:42:56.998524Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"2024-04-16 04:42:40.155567: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-16 04:42:40.155677: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-16 04:42:40.401519: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 272\u001b[0m\n\u001b[1;32m    269\u001b[0m     accelerator\u001b[38;5;241m.\u001b[39mend_training()\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[21], line 47\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m max_grad_norm                   \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     45\u001b[0m diffusion_scheduler             \u001b[38;5;241m=\u001b[39m DDPMScheduler  \n\u001b[0;32m---> 47\u001b[0m accelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfp16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m device      \u001b[38;5;241m=\u001b[39m accelerator\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Load scheduler, tokenizer and unet models.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:371\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, dispatch_batches, even_batches, use_seedable_sampler, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend)\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast_handler \u001b[38;5;241m=\u001b[39m handler\n\u001b[1;32m    370\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[43mAcceleratorState\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmixed_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamo_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamo_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeepspeed_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeepspeed_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfsdp_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsdp_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmegatron_lm_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmegatron_lm_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_from_accelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_recipe_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mmixed_precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp8\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_recipe_handler \u001b[38;5;241m=\u001b[39m FP8RecipeKwargs(backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSAMP\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_msamp_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/state.py:779\u001b[0m, in \u001b[0;36mAcceleratorState.__init__\u001b[0;34m(self, mixed_precision, cpu, dynamo_plugin, deepspeed_plugin, fsdp_plugin, megatron_lm_plugin, _from_accelerator, **kwargs)\u001b[0m\n\u001b[1;32m    777\u001b[0m     PartialState(cpu, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(PartialState\u001b[38;5;241m.\u001b[39m_shared_state)\n\u001b[0;32m--> 779\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized:\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/state.py:878\u001b[0m, in \u001b[0;36mAcceleratorState._check_initialized\u001b[0;34m(self, mixed_precision, cpu)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err\u001b[38;5;241m.\u001b[39mformat(flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu=True\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    874\u001b[0m     mixed_precision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m mixed_precision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mixed_precision\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m    877\u001b[0m ):\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err\u001b[38;5;241m.\u001b[39mformat(flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed_precision=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmixed_precision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n","\u001b[0;31mValueError\u001b[0m: AcceleratorState has already been initialized and cannot be changed, restart your runtime completely and pass `mixed_precision='fp16'` to `Accelerator()`."],"ename":"ValueError","evalue":"AcceleratorState has already been initialized and cannot be changed, restart your runtime completely and pass `mixed_precision='fp16'` to `Accelerator()`.","output_type":"error"}]},{"cell_type":"code","source":"#%%\nfrom diffusers import StableDiffusionPipeline\nimport torch\nfrom diffusers.utils import make_image_grid\nfrom diffusers import EulerDiscreteScheduler\n\nlora_name = \"lora_stable-diffusion-v1-5_rank4_s200_r768_DDPMScheduler_20240214-122106.safetensors\"\nlora_model_path = f\"./output_dir/{lora_name}\"\n\ndevice = \"cuda:0\"\npipe = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\"\n    , torch_dtype = torch.bfloat16\n).to(device)\n\npipe.load_lora_weights(\n    pretrained_model_name_or_path_or_dict=lora_model_path\n    , adapter_name = \"az_lora\"\n)\n\nprompt = \"a toy bike. macro photo. 3d game asset\"\nnagtive_prompt = \"low quality, blur, watermark, words, name\"\n\npipe.set_adapters(\n    [\"az_lora\"]\n    , adapter_weights = [1.0]\n)\n\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n\nimages = pipe(\n    prompt                      = prompt\n    , nagtive_prompt            = nagtive_prompt\n    , num_images_per_prompt     = 4\n    , generator                 = torch.Generator(device).manual_seed(12)\n    , width                     = 768\n    , height                    = 768\n    , guidance_scale            = 8.5\n).images\n\npipe.to(\"cpu\")\ntorch.cuda.empty_cache()\nmake_image_grid(images, cols = 2, rows = 2)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:43:36.955909Z","iopub.execute_input":"2024-04-16T04:43:36.956283Z","iopub.status.idle":"2024-04-16T04:44:53.431763Z","shell.execute_reply.started":"2024-04-16T04:43:36.956252Z","shell.execute_reply":"2024-04-16T04:44:53.430296Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7cef7c1520c403a9c02736c197806ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ad852204c344587a29fff7ebf07b850"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler/scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50a49a63295f44d78a4b588c534c0f43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)ature_extractor/preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be04cd28b1c2419fb76079ca1cc0d196"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"text_encoder/model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b81a3fa5a6e4e27be06bff1b84392a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"safety_checker/model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83bb5791894445838b891f448e044602"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc9f329b1104367a7f9c7b76b50a39f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unet/config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5825b5e4dfe4320b1da2dce758d262d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98dcbb519b0f4f2fa49310eef7fc8d1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vae/config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4368c4c61494e2495f8d7644ded661d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unet/diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47470506a52646dda1393291d3ce7b90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14bad5729ddc473c9fe961b8822f1b3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9559fde23fe84e65a9f9906906ac6ad8"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m pipe \u001b[38;5;241m=\u001b[39m StableDiffusionPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunwayml/stable-diffusion-v1-5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m     , torch_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[1;32m     14\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lora_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path_or_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_model_path\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maz_lora\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma toy bike. macro photo. 3d game asset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m nagtive_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlow quality, blur, watermark, words, name\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/loaders/lora.py:114\u001b[0m, in \u001b[0;36mLoraLoaderMixin.load_lora_weights\u001b[0;34m(self, pretrained_model_name_or_path_or_dict, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     pretrained_model_name_or_path_or_dict \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path_or_dict\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# First, ensure that the checkpoint is a compatible one and can be successfully loaded.\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m state_dict, network_alphas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path_or_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m is_correct_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlora\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_correct_format:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/loaders/lora.py:240\u001b[0m, in \u001b[0;36mLoraLoaderMixin.lora_state_dict\u001b[0;34m(cls, pretrained_model_name_or_path_or_dict, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# Here we're relaxing the loading check to enable more Inference API\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# friendliness where sometimes, it's not at all possible to automatically\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# determine `weight_name`.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m         weight_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_best_guess_weight_name\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path_or_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.safetensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     model_file \u001b[38;5;241m=\u001b[39m _get_model_file(\n\u001b[1;32m    246\u001b[0m         pretrained_model_name_or_path_or_dict,\n\u001b[1;32m    247\u001b[0m         weights_name\u001b[38;5;241m=\u001b[39mweight_name \u001b[38;5;129;01mor\u001b[39;00m LORA_WEIGHT_NAME_SAFE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    257\u001b[0m     )\n\u001b[1;32m    258\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m safetensors\u001b[38;5;241m.\u001b[39mtorch\u001b[38;5;241m.\u001b[39mload_file(model_file, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/loaders/lora.py:323\u001b[0m, in \u001b[0;36mLoraLoaderMixin._best_guess_weight_name\u001b[0;34m(cls, pretrained_model_name_or_path_or_dict, file_extension, local_files_only)\u001b[0m\n\u001b[1;32m    319\u001b[0m     targeted_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    320\u001b[0m         f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(pretrained_model_name_or_path_or_dict) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(file_extension)\n\u001b[1;32m    321\u001b[0m     ]\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 323\u001b[0m     files_in_repo \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path_or_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msiblings\n\u001b[1;32m    324\u001b[0m     targeted_files \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mrfilename \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files_in_repo \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mrfilename\u001b[38;5;241m.\u001b[39mendswith(file_extension)]\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(targeted_files) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:111\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[1;32m    109\u001b[0m ):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 111\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:159\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m     )\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './output_dir/lora_stable-diffusion-v1-5_rank4_s200_r768_DDPMScheduler_20240214-122106.safetensors'. Use `repo_type` argument if needed."],"ename":"HFValidationError","evalue":"Repo id must be in the form 'repo_name' or 'namespace/repo_name': './output_dir/lora_stable-diffusion-v1-5_rank4_s200_r768_DDPMScheduler_20240214-122106.safetensors'. Use `repo_type` argument if needed.","output_type":"error"}]}]}