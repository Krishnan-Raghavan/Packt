{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOubAUXvuB+rvtYsL4Rd1M0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krishnan-Raghavan/Packt/blob/main/GameTheoryChapter9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oBM41-OQoH_N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GridWorld:\n",
        "    def __init__(self, grid_size=(50, 50), num_obstacles=100):\n",
        "        self.grid_size = grid_size\n",
        "        self.num_obstacles = num_obstacles\n",
        "        self.state = (0, 0)  # Start state\n",
        "        self.goal_state = (grid_size[0]-1, grid_size[1]-1)  # Goal state at the bottom-right corner\n",
        "        self.actions = ['up', 'down', 'left', 'right']\n",
        "        self.rewards = np.zeros(grid_size)\n",
        "        self.rewards[self.goal_state] = 1  # Reward for reaching the goal\n",
        "        self.obstacle_states = self._place_obstacles()\n",
        "        for obstacle in self.obstacle_states:\n",
        "            self.rewards[obstacle] = -1  # Penalty for hitting the obstacle\n",
        "\n",
        "    def _place_obstacles(self):\n",
        "        obstacles = set()\n",
        "        while len(obstacles) < self.num_obstacles:\n",
        "            obstacle = (random.randint(0, self.grid_size[0]-1), random.randint(0, self.grid_size[1]-1))\n",
        "            if obstacle != self.state and obstacle != self.goal_state:\n",
        "                obstacles.add(obstacle)\n",
        "        return list(obstacles)\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (0, 0)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = list(self.state)\n",
        "        if action == 0:  # Up\n",
        "            next_state[0] = max(0, self.state[0] - 1)\n",
        "        elif action == 1:  # Down\n",
        "            next_state[0] = min(self.grid_size[0] - 1, self.state[0] + 1)\n",
        "        elif action == 2:  # Left\n",
        "            next_state[1] = max(0, self.state[1] - 1)\n",
        "        elif action == 3:  # Right\n",
        "            next_state[1] = min(self.grid_size[1] - 1, self.state[1] + 1)\n",
        "\n",
        "        self.state = tuple(next_state)\n",
        "        reward = self.rewards[self.state]\n",
        "        done = self.state == self.goal_state\n",
        "        return self.state, reward, done\n",
        "\n",
        "    def print_grid(self):\n",
        "        grid = np.full(self.grid_size, ' ')\n",
        "        grid[self.state] = 'S'\n",
        "        grid[self.goal_state] = 'G'\n",
        "        for obstacle in self.obstacle_states:\n",
        "            grid[obstacle] = 'X'\n",
        "\n",
        "        print('\\n'.join(' '.join(row) for row in grid))"
      ],
      "metadata": {
        "id": "m40OCknwobTp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, num_episodes=1000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "    q_table = np.zeros((*env.grid_size, len(env.actions)))\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = random.choice(range(len(env.actions)))  # Explore: random action\n",
        "            else:\n",
        "                action = np.argmax(q_table[state])  # Exploit: best action based on Q-table\n",
        "\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            old_value = q_table[state][action]\n",
        "            next_max = np.max(q_table[next_state])\n",
        "\n",
        "            # Update Q-value using the Bellman equation\n",
        "            q_table[state][action] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return q_table"
      ],
      "metadata": {
        "id": "G1Tpq5fcogcx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize environment with a 8x8 grid and 16 obstacles\n",
        "grid_size = (5, 5)\n",
        "num_obstacles = 5\n",
        "env = GridWorld(grid_size, num_obstacles)\n",
        "\n",
        "# Print the initial grid\n",
        "print(\"Initial Grid:\")\n",
        "env.print_grid()\n",
        "\n",
        "# Run Q-learning\n",
        "q_table = q_learning(env)\n",
        "\n",
        "# Print the Q-table\n",
        "print(\"Q-Table:\")\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfWnpm_polgK",
        "outputId": "2c60e7a8-1978-4c9d-c27d-283ffe889b20"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Grid:\n",
            "S       X\n",
            "X   X    \n",
            "  X      \n",
            "         \n",
            "      X G\n",
            "Q-Table:\n",
            "[[[ 0.32296569 -0.67303051  0.42231948  0.4782969 ]\n",
            "  [ 0.44361408  0.32552745  0.40736329  0.531441  ]\n",
            "  [ 0.49998303 -0.60276012  0.4463094   0.59049   ]\n",
            "  [ 0.54225082  0.6561      0.50848162 -0.39952455]\n",
            "  [-0.39901955  0.728302    0.         -0.5017291 ]]\n",
            "\n",
            " [[ 0.41460793  0.         -0.40951     0.0340037 ]\n",
            "  [ 0.45275674 -0.61257951 -0.45708339 -0.47850578]\n",
            "  [ 0.52767649  0.          0.02414001  0.        ]\n",
            "  [ 0.52847381  0.50691927 -0.54991743  0.729     ]\n",
            "  [-0.37483899  0.81        0.63615757  0.60807109]]\n",
            "\n",
            " [[-0.3439      0.          0.         -0.1       ]\n",
            "  [ 0.039154    0.          0.          0.        ]\n",
            "  [-0.1         0.          0.          0.        ]\n",
            "  [ 0.65561482  0.00713231  0.          0.1538071 ]\n",
            "  [ 0.61957728  0.9         0.51712337  0.78205593]]\n",
            "\n",
            " [[ 0.          0.          0.          0.        ]\n",
            "  [-0.1         0.          0.          0.        ]\n",
            "  [ 0.          0.          0.          0.        ]\n",
            "  [ 0.51479849  0.          0.          0.        ]\n",
            "  [ 0.76747379  1.          0.32114803  0.84766063]]\n",
            "\n",
            " [[ 0.          0.          0.          0.        ]\n",
            "  [ 0.          0.          0.          0.        ]\n",
            "  [ 0.          0.          0.          0.        ]\n",
            "  [ 0.          0.          0.          0.        ]\n",
            "  [ 0.          0.          0.          0.        ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract policy\n",
        "policy = np.argmax(q_table, axis=2)\n",
        "policy_symbols = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
        "\n",
        "print(\"\\nPolicy:\")\n",
        "for row in policy:\n",
        "    print(' '.join(policy_symbols[action] for action in row))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQiC6gu-onTl",
        "outputId": "e23014c9-e01c-4e1e-ba46-c293a241f018"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Policy:\n",
            "→ → → ↓ ↓\n",
            "↑ ↑ ↑ → ↓\n",
            "↓ ↑ ↓ ↑ ↓\n",
            "↑ ↓ ↑ ↑ ↓\n",
            "↑ ↑ ↑ ↑ ↑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the learned policy\n",
        "for _ in range(5):\n",
        "    state = env.reset()\n",
        "    path = [state]\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        next_state, reward, done = env.step(action)\n",
        "        path.append(next_state)\n",
        "        state = next_state\n",
        "    print(f\"Path taken: {path}\")\n",
        "    print(\"Goal reached!\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rt1iAhdoudw",
        "outputId": "f5ce5528-021d-492d-e646-76479526c343"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path taken: [(0, 0), (0, 1), (0, 2), (0, 3), (1, 3), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
            "Goal reached!\n",
            "\n",
            "Path taken: [(0, 0), (0, 1), (0, 2), (0, 3), (1, 3), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
            "Goal reached!\n",
            "\n",
            "Path taken: [(0, 0), (0, 1), (0, 2), (0, 3), (1, 3), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
            "Goal reached!\n",
            "\n",
            "Path taken: [(0, 0), (0, 1), (0, 2), (0, 3), (1, 3), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
            "Goal reached!\n",
            "\n",
            "Path taken: [(0, 0), (0, 1), (0, 2), (0, 3), (1, 3), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
            "Goal reached!\n",
            "\n"
          ]
        }
      ]
    }
  ]
}